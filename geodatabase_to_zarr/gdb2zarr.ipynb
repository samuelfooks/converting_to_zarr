{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a83aade",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef89c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import dask\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c546d2",
   "metadata": {},
   "source": [
    "## Updating Dataset Attributes\n",
    "\n",
    "The function `attributes_update` is used to update the attributes of a dataset. It takes four parameters: `dataset`, `title`, `resolution`, and `zipurl`.\n",
    "\n",
    "```python\n",
    "def attributes_update(dataset, title, resolution, zipurl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb4483",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def attributes_update(dataset, title, resolution, zipurl):\n",
    "        latitudeattrs = {'_CoordinateAxisType': 'Lat', \n",
    "                            'axis': 'Y', \n",
    "                            'long_name': 'latitude', \n",
    "                            'max': dataset.latitude.values.max(), \n",
    "                            'min': dataset.latitude.values.min(), \n",
    "                            'standard_name': 'latitude', \n",
    "                            'step': (dataset.latitude.values.max() - dataset.latitude.values.min()) / dataset.latitude.values.shape[0], \n",
    "                            'units': 'degrees_north'\n",
    "            }\n",
    "        longitudeattrs = {'_CoordinateAxisType': 'Lon', \n",
    "                        'axis': 'X', \n",
    "                        'long_name': 'longitude',\n",
    "                        'max': dataset.longitude.values.max(),\n",
    "                        'min': dataset.longitude.values.min(),\n",
    "                        'standard_name': 'longitude', \n",
    "                        'step': (dataset.longitude.values.max() - dataset.longitude.values.min()) / dataset.longitude.values.shape[0], \n",
    "                        'units': 'degrees_east'\n",
    "        }\n",
    "        dataset.latitude.attrs.update(latitudeattrs)\n",
    "        dataset.longitude.attrs.update(longitudeattrs)\n",
    "\n",
    "        # Set the CRS as an attribute\n",
    "        dataset.attrs['proj:epsg'] = 4326\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        dataset.attrs.update({\n",
    "            'geospatial_lat_min': dataset['latitude'].min().item(),\n",
    "            'geospatial_lat_max': dataset['latitude'].max().item(),\n",
    "            'geospatial_lon_min': dataset['longitude'].min().item(),\n",
    "            'geospatial_lon_max': dataset['longitude'].max().item()\n",
    "        })\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        #include where the data comes and when its been converted\n",
    "        dataset.attrs['History'] = f'Zarr dataset converted from {title}.gdb, downloaded from {zipurl}, on {date.today()}'\n",
    "        \n",
    "        #add any other attributes you think necessary to include in the metadata of your zarr dataset\n",
    "        #dataset.attrs['sources'] = source\n",
    "    \n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac55a8",
   "metadata": {},
   "source": [
    "## Function: `gdf2zarrconverter`\n",
    "\n",
    "This function converts a geodataframe to a Zarr array. It accepts six parameters: `file_path`, `native_var`, `title`, `layer`, `arco_asset_tmp_path`, and `zipurl`.\n",
    "\n",
    "The function first defines two helper functions, `cleaner` and `encode_categorical`. `cleaner` is used to clean the data by replacing certain values with 'None'. `encode_categorical` is used to encode categorical data into numerical values.\n",
    "\n",
    "The function then opens the file at `file_path` and reads its contents. It calculates the bounds and resolution of the data, and creates an empty raster of the appropriate size.\n",
    "\n",
    "Next, the function iterates over the features in the source data. For each feature, it cleans the data and appends it to a list. It also appends the feature's geometry to a separate list.\n",
    "\n",
    "The data is then encoded using the `encode_categorical` function. The geometries and encoded data are used to rasterize the data.\n",
    "\n",
    "An xarray dataset is created from the raster, and the latitude is sorted. If there is a category mapping, it is saved as an attribute of the dataset.\n",
    "\n",
    "Finally, the dataset is updated with the `attributes_update` function, saved as a Zarr array at `zarr_var_path`, and the path is returned.\n",
    "\n",
    "```python\n",
    "def gdf2zarrconverter(file_path, native_var, title, layer, arco_asset_tmp_path, zipurl):\n",
    "    ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf2zarrconverter(file_path, native_var, title, layer, arco_asset_tmp_path, zipurl):\n",
    "\n",
    "    def cleaner(data):\n",
    "        if isinstance(data, str):\n",
    "            if data == '0' or data == ' ' or data == np.nan or data == 'nan' or data == \"\" or data == \" \":\n",
    "                data = 'None'\n",
    "        return data\n",
    "\n",
    "    def encode_categorical(data):\n",
    "        if isinstance(data[0], str):\n",
    "            data = pd.Series(data)\n",
    "            data = data.fillna('None')  # replace None values with 'None'\n",
    "            \n",
    "            data[data == ' '] = 'None'\n",
    "            data[data == '0'] = 'None'\n",
    "            data = data.values \n",
    "            unique_categories = np.unique(data)\n",
    "            category_mapping = {'None': 1}\n",
    "            counter = 2\n",
    "            for category in unique_categories:\n",
    "                if category != 'None':\n",
    "                    category_mapping[category] = counter\n",
    "                    counter += 1\n",
    "            encoded_data = np.array([category_mapping.get(item, np.nan) for item in data])\n",
    "        else:\n",
    "            encoded_data = data.astype(np.float32)\n",
    "            category_mapping = {}\n",
    "        return encoded_data, category_mapping\n",
    "\n",
    "    with fiona.open(file_path, 'r', layer=layer) as src:\n",
    "        crs = src.crs\n",
    "        total_bounds = src.bounds\n",
    "        lon_min, lat_min, lon_max, lat_max = total_bounds\n",
    "        resolution = 0.01\n",
    "        width = int(np.ceil((lon_max - lon_min) / resolution))\n",
    "        height = int(np.ceil((lat_max - lat_min) / resolution))\n",
    "        raster_transform = rasterio.transform.from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "        raster = np.zeros((height, width), dtype=np.float32)\n",
    "        data = []\n",
    "        geometries = []\n",
    "        with tqdm(total=len(src), desc=f\"Processing features of {layer} - {native_var}\") as pbar:\n",
    "            for feature in src:\n",
    "                value = cleaner(feature['properties'][native_var])\n",
    "                data.append(value)\n",
    "                geometries.append(feature['geometry'])\n",
    "                pbar.update()\n",
    "        data = np.array(data)\n",
    "        encoded_data, category_mapping = encode_categorical(data)\n",
    "        with tqdm(total=len(geometries), desc=\"Rasterizing\") as pbar:\n",
    "            rasterio.features.rasterize(\n",
    "                ((geom, value) for geom, value in zip(geometries, encoded_data)),\n",
    "                out=raster,\n",
    "                transform=raster_transform,\n",
    "                merge_alg=rasterio.enums.MergeAlg.replace,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            pbar.update()\n",
    "        \n",
    "        # make xarray dataset, arrange latitude from max to min since rasterio makes rasters from top left to bottom right\n",
    "        dataset = xr.Dataset(coords={'latitude':  np.round(np.linspace(lat_max, lat_min, height, dtype=float), decimals=4),\n",
    "                                    'longitude': np.round(np.linspace(lon_min, lon_max, width, dtype=float), decimals=4)})\n",
    "        dataset[native_var] = (['latitude', 'longitude'], raster)\n",
    "        dataset = dataset.sortby('latitude')\n",
    "\n",
    "        if category_mapping:\n",
    "            # save the mappig dictionary with the variable attributes\n",
    "            dataset[native_var].attrs['categorical_encoding']= category_mapping\n",
    "\n",
    "        dataset = attributes_update(dataset, title, resolution, zipurl)\n",
    "        zarr_var_path = f\"{arco_asset_tmp_path}/{title}_{native_var}.zarr\"\n",
    "        dataset.to_zarr(zarr_var_path, mode='w', consolidated=True)\n",
    "        return zarr_var_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ed820",
   "metadata": {},
   "source": [
    "## Downloading and Extracting a Zip File\n",
    "\n",
    "This Python code snippet is used to download a zip file from a specified URL, extract it, and find a specfic geodatabase. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6c378",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Download the zip file\n",
    "zipurl = 'https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/emodnet_geology/seabed_substrate/multiscale_folk_5/EMODnet_GEO_Seabed_Substrate_All_Res.zip'\n",
    "geodatabase = 'EMODnet_Seabed_Substrate_1M.gdb'\n",
    "zip_file = os.path.basename(zipurl)\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=zip_file) as t:\n",
    "    urllib.request.urlretrieve(zipurl, filename=zip_file, reporthook=t.update_to)\n",
    "    # Extract the geodatabase from the zip file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('extracted_files')\n",
    "for root, dirs, files in os.walk('extracted_files'):\n",
    "    for dir in dirs:\n",
    "        if dir.endswith('.gdb') and os.path.basename(dir) == geodatabase:\n",
    "            gdb_path = os.path.join(root, dir)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f86dc",
   "metadata": {},
   "source": [
    "## Convert to Zarr\n",
    "\n",
    "This Python code snippet goes through each layer and converts each variable of each layer into Zarr format, using the gdf2zarrconverter.  This is to minimize memory consumption during the conversion process.  Then each dataset is rechunked and combined into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa57981",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_zarr_path = 'converted_zarr_files'\n",
    "os.makedirs(temp_zarr_path, exist_ok=True)\n",
    "title = os.path.splitext(os.path.basename(geodatabase))[0]\n",
    "\n",
    "\n",
    "# Get the layers from the geodatabase\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "\n",
    "# Create an empty xarray dataset to hold the combined data\n",
    "combined_dataset = xr.Dataset()\n",
    "\n",
    "# Process each layer and each variable using gdf2zarr\n",
    "for layer in layers:\n",
    "    # Get the variables from the layer\n",
    "    variables = fiona.open(gdb_path, layer=layer).meta['schema']['properties'].keys()\n",
    "    \n",
    "    zarr_vars_paths = [] # replace with your column names\n",
    "    for variable in variables:\n",
    "        try:\n",
    "            print(f\"Processing {layer} - {variable}\")\n",
    "            zarr_var_path = gdf2zarrconverter(gdb_path, variable, title, layer, temp_zarr_path, zipurl)\n",
    "            zarr_vars_paths.append(zarr_var_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {layer} - {variable}: {e}\")\n",
    "            continue\n",
    "\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        for path in zarr_vars_paths:\n",
    "            try:\n",
    "                dataset = xr.open_dataset(path, chunks={})  # Use Dask to lazily load the dataset\n",
    "                dataset = dataset.chunk({'latitude': 'auto', 'longitude': 'auto'}) \n",
    "                combined_dataset = xr.merge([combined_dataset, dataset], compat='override', join='outer')\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to combine zarr dataset {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # add applicable categorical encodings\n",
    "    categorical_encodings_dict = {}\n",
    "    for var in combined_dataset.variables:\n",
    "        if 'categorical_encoding' in combined_dataset[var].attrs:\n",
    "            categorical_encodings_dict[var] = combined_dataset[var].attrs['categorical_encoding']\n",
    "\n",
    "    combined_dataset.attrs['categorical_encoding'] = categorical_encodings_dict\n",
    "\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        try:    \n",
    "            final_dataset = combined_dataset.chunk({'latitude': 'auto', 'longitude': 'auto'})  # for var in dataset.variables:\n",
    "            zarr_path = f\"{layer}.zarr\"\n",
    "            final_dataset.to_zarr(zarr_path, mode = 'w')\n",
    "            shutil.rmtree(temp_zarr_path)\n",
    "        except Exception as e:\n",
    "            print(f\"final zarr dataset did not save {layer}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Print the combined dataset\n",
    "print(combined_dataset)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
