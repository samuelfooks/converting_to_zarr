{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a83aade",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b47694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r '../requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdc3fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMODnet_GEO_Seabed_Substrate_All_Res.zip:  54%|█████▍    | 423M/777M [00:04<00:03, 96.1MB/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 350\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# Print the combined dataset\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mprint\u001b[39m(combined_dataset)\n\u001b[0;32m--> 350\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeodatabase\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 287\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(zipurl, geodatabase)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(zipurl, geodatabase):\n\u001b[0;32m--> 287\u001b[0m     gdb_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeodatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     temp_zarr_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverted_zarr_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    289\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(temp_zarr_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 44\u001b[0m, in \u001b[0;36mdownload_extract\u001b[0;34m(zipurl, geodatabase)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(b \u001b[38;5;241m*\u001b[39m bsize \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TqdmUpTo(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, miniters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39mzip_file) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Extract the geodatabase from the zip file\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n",
      "File \u001b[0;32m~/miniforge3/envs/edito-pipeline-env/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/edito-pipeline-env/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniforge3/envs/edito-pipeline-env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/edito-pipeline-env/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/envs/edito-pipeline-env/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date\n",
    "import dask.array as da\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import dask\n",
    "import shutil\n",
    "import shapely\n",
    "from shapely import intersects\n",
    "from shapely.geometry import box\n",
    "from rasterio.windows import Window\n",
    "import matplotlib.pyplot as plt\n",
    "from affine import Affine\n",
    "\n",
    "\n",
    "# zipurl = 'https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/emodnet_human_activities/energy/wind_farms_points/EMODnet_HA_Energy_WindFarms_20240508.zip'\n",
    "# geodatabase = 'EMODnet_HA_Energy_WindFarms_20240508.gdb'\n",
    "\n",
    "zipurl = 'https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/emodnet_geology/seabed_substrate/multiscale_folk_5/EMODnet_GEO_Seabed_Substrate_All_Res.zip'\n",
    "geodatabase = 'EMODnet_Seabed_Substrate_1M.gdb'\n",
    "\n",
    "# zipurl = 'https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/emodnet_seabed_habitats/seabed_habitats_maps/habitat_types_in_euseamap_2023_eunis_2019/EUSeaMap_2023.zip'\n",
    "# geodatabase = 'EUSeaMap_2023.gdb'\n",
    "\n",
    "\n",
    "\n",
    "def download_extract(zipurl, geodatabase):\n",
    "    zip_file = os.path.basename(zipurl)\n",
    "    class TqdmUpTo(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "    with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=zip_file) as t:\n",
    "        urllib.request.urlretrieve(zipurl, filename=zip_file, reporthook=t.update_to)\n",
    "        # Extract the geodatabase from the zip file\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('extracted_files')\n",
    "    for root, dirs, files in os.walk('extracted_files'):\n",
    "        for dir in dirs:\n",
    "            if dir.endswith('.gdb') and os.path.basename(dir) == geodatabase:\n",
    "                gdb_path = os.path.join(root, dir)\n",
    "                return gdb_path\n",
    "                break\n",
    "\n",
    "def attributes_update(dataset, title, resolution, zipurl):\n",
    "        latitudeattrs = {'_CoordinateAxisType': 'Lat', \n",
    "                            'axis': 'Y', \n",
    "                            'long_name': 'latitude', \n",
    "                            'max': dataset.latitude.values.max(), \n",
    "                            'min': dataset.latitude.values.min(), \n",
    "                            'standard_name': 'latitude', \n",
    "                            'step': (dataset.latitude.values.max() - dataset.latitude.values.min()) / dataset.latitude.values.shape[0], \n",
    "                            'units': 'degrees_north'\n",
    "            }\n",
    "        longitudeattrs = {'_CoordinateAxisType': 'Lon', \n",
    "                        'axis': 'X', \n",
    "                        'long_name': 'longitude',\n",
    "                        'max': dataset.longitude.values.max(),\n",
    "                        'min': dataset.longitude.values.min(),\n",
    "                        'standard_name': 'longitude', \n",
    "                        'step': (dataset.longitude.values.max() - dataset.longitude.values.min()) / dataset.longitude.values.shape[0], \n",
    "                        'units': 'degrees_east'\n",
    "        }\n",
    "        dataset.latitude.attrs.update(latitudeattrs)\n",
    "        dataset.longitude.attrs.update(longitudeattrs)\n",
    "\n",
    "        # Set the CRS as an attribute\n",
    "        dataset.attrs['proj:epsg'] = 4326\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        dataset.attrs.update({\n",
    "            'geospatial_lat_min': dataset['latitude'].min().item(),\n",
    "            'geospatial_lat_max': dataset['latitude'].max().item(),\n",
    "            'geospatial_lon_min': dataset['longitude'].min().item(),\n",
    "            'geospatial_lon_max': dataset['longitude'].max().item()\n",
    "        })\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        #include where the data comes and when its been converted\n",
    "        dataset.attrs['History'] = f'Zarr dataset converted from {title}.gdb, downloaded from {zipurl}, on {date.today()}'\n",
    "        \n",
    "        #add any other attributes you think necessary to include in the metadata of your zarr dataset\n",
    "        #dataset.attrs['sources'] = source\n",
    "    \n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def rasterize_chunk(chunk, raster_shape, transform):\n",
    "    raster_chunk = np.zeros(raster_shape, dtype=np.float32)\n",
    "\n",
    "    valid_geoms = []\n",
    "    for geom in chunk['geometries']:\n",
    "        if geom.is_valid:\n",
    "            valid_geoms.append(geom)\n",
    "    if not valid_geoms:\n",
    "        return raster_chunk\n",
    "    shapes = ((geom, value) for geom, value in zip(valid_geoms, chunk['encoded_data']))\n",
    "    rasterio.features.rasterize(\n",
    "        shapes,\n",
    "        out=raster_chunk,\n",
    "        transform=transform,\n",
    "        merge_alg=rasterio.enums.MergeAlg.replace,\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    \n",
    "    return raster_chunk\n",
    "\n",
    "\n",
    "def cleaner(data):\n",
    "    if isinstance(data, str):\n",
    "        if data == '0' or data.strip() == '' or pd.isna(data):\n",
    "            data = 'None'\n",
    "    return data\n",
    "\n",
    "def encode_categorical(data, category_mapping):\n",
    "    if isinstance(data[0], str):\n",
    "        data = pd.Series(data).fillna('None')\n",
    "        data[data == ' '] = 'None'\n",
    "        data[data == ''] = 'None'\n",
    "        data[data == '0'] = 'None'\n",
    "        data = data.values \n",
    "        unique_categories = np.unique(data)\n",
    "        for category in unique_categories:\n",
    "            if category not in category_mapping:\n",
    "                category_mapping[category] = len(category_mapping) + 1\n",
    "        # category_mapping = {cat: i + 1 for i, cat in enumerate(unique_categories)}\n",
    "        encoded_data = np.array([category_mapping[item] for item in data])\n",
    "    else:\n",
    "        # Convert to float32\n",
    "        data = data.astype(np.float32)\n",
    "        # If there are any NaN values, replace them with None\n",
    "        if np.isnan(data).any():\n",
    "           data[np.isnan(data)] = 0\n",
    "        encoded_data = data\n",
    "        \n",
    "    return encoded_data, category_mapping\n",
    "\n",
    "\n",
    "def plot_raster(raster):\n",
    "    # Compute the raster\n",
    "    raster_computed = raster.compute()\n",
    "\n",
    "    # Plot the raster\n",
    "    plt.imshow(raster_computed, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Raster')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gdf2zarrconverter(file_path, native_var, title, layer, arco_asset_tmp_path, metadata_dict):\n",
    "    with fiona.open(file_path, 'r', layer=layer) as src:\n",
    "        crs = src.crs\n",
    "        total_bounds = src.bounds\n",
    "        lon_min, lat_min, lon_max, lat_max = total_bounds\n",
    "        resolution = 0.001  # Adjust resolution as needed\n",
    "        width = int(np.ceil((lon_max - lon_min) / resolution))\n",
    "        height = int(np.ceil((lat_max - lat_min) / resolution))\n",
    "        raster_transform = rasterio.transform.from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "\n",
    "        chunk_width = 2000\n",
    "        chunk_height = 2000\n",
    "        category_mapping = {}\n",
    "        lazy_rasters = []\n",
    "        dataset = xr.Dataset()\n",
    "        # with tqdm(total=((height + chunk_height - 1) // chunk_height) * ((width + chunk_width - 1) // chunk_width), desc=f\"Processing windows of {layer} - {native_var}\") as pbar_window:\n",
    "        row_num= 0\n",
    "        for i in range(0, height, chunk_height):\n",
    "            remainder_y = height - i\n",
    "            window_height = min(chunk_height, remainder_y)\n",
    "            row_rasters = []\n",
    "            for j in range(0, width, chunk_width):\n",
    "                print(f\"Processing window at i={i}, from width{width}, j={j}, from height{height}\")\n",
    "                remainder_x = width - j\n",
    "                window_width = min(chunk_width, remainder_x)\n",
    "                window = Window(j, i, window_width, window_height)\n",
    "                \n",
    "                chunk_geoms = []\n",
    "                chunk_data = []\n",
    "                window_geom = box(\n",
    "                    *rasterio.windows.bounds(window, transform=raster_transform)\n",
    "                )\n",
    "                with tqdm(total=len(src), desc=f\"Processing features of {layer} - {native_var}\") as pbar:\n",
    "                    for feature in src:\n",
    "                        geom = feature['geometry']\n",
    "                        if isinstance(geom, fiona.model.Geometry):\n",
    "                            geom = shapely.geometry.shape(geom)\n",
    "                        if geom.intersects(window_geom):\n",
    "                            chunk_geoms.append(geom)\n",
    "                            # Only call the cleaner function if the data type is a string\n",
    "                            if isinstance(feature['properties'][native_var], str):\n",
    "                                chunk_data.append(cleaner(feature['properties'][native_var]))\n",
    "                            else:\n",
    "                                chunk_data.append(feature['properties'][native_var])\n",
    "                        pbar.update(1)\n",
    "\n",
    "                if not chunk_geoms:\n",
    "                    # Create an empty raster\n",
    "                    empty_raster = np.zeros((window_height, window_width))\n",
    "                    lazy_raster = da.from_array(empty_raster, chunks=(chunk_height, chunk_width))\n",
    "                    \n",
    "                else:\n",
    "                    chunk_data = np.array(chunk_data)\n",
    "                    encoded_data, category_mapping = encode_categorical(chunk_data, category_mapping) \n",
    "\n",
    "                    # Calculate the top-left corner of the chunk\n",
    "                    chunk_x = raster_transform.c + window.col_off * raster_transform.a\n",
    "                    chunk_y = raster_transform.f + window.row_off * raster_transform.e\n",
    "\n",
    "                    # Create a new transform for the chunk\n",
    "                    chunk_transform = Affine(raster_transform.a, raster_transform.b, chunk_x,\n",
    "                                            raster_transform.d, raster_transform.e, chunk_y)\n",
    "\n",
    "                    # Use the chunk_transform in the delayed function\n",
    "                    lazy_raster = dask.delayed(rasterize_chunk)(\n",
    "                        {'geometries': chunk_geoms, 'encoded_data': encoded_data},\n",
    "                        (window_height, window_width),\n",
    "                        chunk_transform\n",
    "                    )\n",
    "                    lazy_raster = da.from_delayed(lazy_raster, shape=(window_height, window_width), dtype=np.float32)\n",
    "                                        \n",
    "                row_rasters.append(lazy_raster)\n",
    "\n",
    "            # Instead of appending to row_rasters, create a DataArray and add it to the Dataset\n",
    "            if row_rasters:\n",
    "                row_raster = da.concatenate(row_rasters, axis=1)\n",
    "                row_raster_computed = row_raster.compute()  # Compute the raster to check values\n",
    "                print(np.max(row_raster_computed))\n",
    "                data_array = xr.DataArray(\n",
    "                    row_raster,\n",
    "                    dims=['latitude', 'longitude'],\n",
    "                    name=native_var\n",
    "                ).chunk({'latitude': 500, 'longitude': 500})\n",
    "            \n",
    "            # Create the Dataset with native_var as a key, and the data and coordinates associated with this key\n",
    "            dataset = xr.Dataset(\n",
    "                {native_var: (['latitude', 'longitude'], data_array.data)},\n",
    "                coords={'latitude': data_array.latitude, 'longitude': data_array.longitude}\n",
    "            )\n",
    "\n",
    "            data_array.to_zarr(f'converted_zarr_files/rows/row_{row_num}.zarr', mode='w')\n",
    "            row_num += 1\n",
    "            print(f\"Finished processing window at i={i}\")\n",
    "\n",
    "        # Load the zarr files and concatenate them\n",
    "        datasets = [xr.open_zarr(f'converted_zarr_files/rows/row_{i}.zarr') for i in range(row_num)]\n",
    "        dataset = xr.concat(datasets, dim='latitude')\n",
    "\n",
    "        latitudes = np.round(np.linspace(lat_max, lat_min, height, dtype=float), decimals=4)\n",
    "        longitudes = np.round(np.linspace(lon_min, lon_max, width, dtype=float), decimals=4)\n",
    "\n",
    "        dataset = dataset.assign_coords({'latitude': latitudes, 'longitude': longitudes})\n",
    "        dataset = dataset.sortby('latitude')\n",
    "        dataset = dataset.chunk({'latitude': 'auto', 'longitude': 'auto'})\n",
    "\n",
    "        if category_mapping:\n",
    "            dataset[native_var].attrs['categorical_encoding'] = category_mapping\n",
    "            dataset.attrs['categorical_encoding'] = {native_var: category_mapping}\n",
    "        dataset = attributes_update(dataset, title, resolution, metadata_dict)\n",
    "            \n",
    "        zarr_var_path = f\"{arco_asset_tmp_path}/{title}_{native_var}.zarr\"\n",
    "        dataset.to_zarr(zarr_var_path, mode='w', consolidated=True)\n",
    "        return zarr_var_path\n",
    "\n",
    "\n",
    "def main(zipurl, geodatabase):\n",
    "    gdb_path = download_extract(zipurl, geodatabase)\n",
    "    temp_zarr_path = 'converted_zarr_files'\n",
    "    os.makedirs(temp_zarr_path, exist_ok=True)\n",
    "    title = os.path.splitext(os.path.basename(geodatabase))[0]\n",
    "\n",
    "    # Get the layers from the geodatabase\n",
    "    layers = fiona.listlayers(gdb_path)\n",
    "\n",
    "    # Create an empty xarray dataset to hold the combined data\n",
    "    combined_dataset = xr.Dataset()\n",
    "\n",
    "     # Process each layer and each variable using gdf2zarr\n",
    "    for layer in layers:\n",
    "        # Get the variables from the layer\n",
    "\n",
    "        # if layer == 'EMODnet_HA_Energy_WindFarms_pg_20240508':\n",
    "\n",
    "        variables = fiona.open(gdb_path, layer=layer).meta['schema']['properties'].keys()\n",
    "        \n",
    "        zarr_vars_paths = [] # replace with your column names\n",
    "        for variable in variables:\n",
    "\n",
    "            # if variable == 'POWER_MW':\n",
    "            try:\n",
    "                print(f\"Processing {layer} - {variable}\")\n",
    "                zarr_var_path = gdf2zarrconverter(gdb_path, variable, title, layer, temp_zarr_path, zipurl)\n",
    "                zarr_vars_paths.append(zarr_var_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {layer} - {variable}: {e}\")\n",
    "                continue\n",
    "\n",
    "        with dask.config.set(scheduler='single-threaded'):\n",
    "            for path in zarr_vars_paths:\n",
    "                try:\n",
    "                    dataset = xr.open_dataset(path, chunks={})  # Use Dask to lazily load the dataset\n",
    "                    dataset = dataset.chunk({'latitude': 'auto', 'longitude': 'auto'}) \n",
    "                    combined_dataset = xr.merge([combined_dataset, dataset], compat='override', join='outer')\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to combine zarr dataset {path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # add applicable categorical encodings\n",
    "        categorical_encodings_dict = {}\n",
    "        for var in combined_dataset.variables:\n",
    "            if 'categorical_encoding' in combined_dataset[var].attrs:\n",
    "                categorical_encodings_dict[var] = combined_dataset[var].attrs['categorical_encoding']\n",
    "\n",
    "        combined_dataset.attrs['categorical_encoding'] = categorical_encodings_dict\n",
    "\n",
    "        with dask.config.set(scheduler='single-threaded'):\n",
    "            try:    \n",
    "                final_dataset = combined_dataset.chunk({'latitude': 'auto', 'longitude': 'auto'})  # for var in dataset.variables:\n",
    "                zarr_path = f\"{layer}.zarr\"\n",
    "                final_dataset.to_zarr(zarr_path, mode = 'w')\n",
    "                shutil.rmtree(temp_zarr_path)\n",
    "            except Exception as e:\n",
    "                print(f\"final zarr dataset did not save {layer}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Print the combined dataset\n",
    "    print(combined_dataset)\n",
    "\n",
    "\n",
    "main(zipurl, geodatabase)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "convertzarrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
