{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Rasterized ARCO Version of a Geodatabase\n",
    "\n",
    "This Jupyter Notebook guides you through the process of converting a geodatabase into a rasterized ARCO version. The geodatabase contains multiple layers of geological seabed substrate data obtained from EMODnet Geology.\n",
    "\n",
    "The conversion process is divided into several steps, each utilizing different Python packages:\n",
    "\n",
    "1. **Reading Geospatial Data**: We use the `fiona` package to read the geospatial data from the geodatabase.\n",
    "\n",
    "2. **Data Manipulation**: The `geopandas` package allows us to manipulate the vector geospatial data as needed.\n",
    "\n",
    "3. **Raster Operations**: We use the `rasterio` package to perform raster operations to rasterize the geodataframes.\n",
    "\n",
    "4. **Working with Multi-dimensional Arrays**: The `xarray` package enables us to work with multi-dimensional arrays, which is crucial for handling geospatial data.\n",
    "\n",
    "5. **Data Storage**: Finally, we use the `zarr` package to store the processed data in a compressed format.  And we store the data in s3 storage, allowing us to subset the data in the cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import dask\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the attributes of a dataset, the `attributes_update()` function is used. This function takes in the dataset, title, resolution, and metadata dictionary as input. It updates the latitude and longitude attributes, sets the CRS, adds spatial extent information, includes the resolution, history, title, comment, and sources attributes. The function ensures that the dataset has accurate and informative attributes for better understanding and analysis of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_update(dataset, title, resolution, zipurl):\n",
    "        latitudeattrs = {'_CoordinateAxisType': 'Lat', \n",
    "                            'axis': 'Y', \n",
    "                            'long_name': 'latitude', \n",
    "                            'max': dataset.latitude.values.max(), \n",
    "                            'min': dataset.latitude.values.min(), \n",
    "                            'standard_name': 'latitude', \n",
    "                            'step': (dataset.latitude.values.max() - dataset.latitude.values.min()) / dataset.latitude.values.shape[0], \n",
    "                            'units': 'degrees_north'\n",
    "            }\n",
    "        longitudeattrs = {'_CoordinateAxisType': 'Lon', \n",
    "                        'axis': 'X', \n",
    "                        'long_name': 'longitude',\n",
    "                        'max': dataset.longitude.values.max(),\n",
    "                        'min': dataset.longitude.values.min(),\n",
    "                        'standard_name': 'longitude', \n",
    "                        'step': (dataset.longitude.values.max() - dataset.longitude.values.min()) / dataset.longitude.values.shape[0], \n",
    "                        'units': 'degrees_east'\n",
    "        }\n",
    "        dataset.latitude.attrs.update(latitudeattrs)\n",
    "        dataset.longitude.attrs.update(longitudeattrs)\n",
    "\n",
    "        # Set the CRS as an attribute\n",
    "        dataset.attrs['proj:epsg'] = 4326\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        dataset.attrs.update({\n",
    "            'geospatial_lat_min': dataset['latitude'].min().item(),\n",
    "            'geospatial_lat_max': dataset['latitude'].max().item(),\n",
    "            'geospatial_lon_min': dataset['longitude'].min().item(),\n",
    "            'geospatial_lon_max': dataset['longitude'].max().item()\n",
    "        })\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        #include where the data comes and when its been converted\n",
    "        dataset.attrs['History'] = f'Zarr dataset converted from {title}.gdb, downloaded from {zipurl}, on {date.today()}'\n",
    "        \n",
    "        #add any other attributes you think necessary to include in the metadata of your zarr dataset\n",
    "        #dataset.attrs['sources'] = source\n",
    "    \n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Converting GeoDataFrame to Zarr Format\n",
    "\n",
    "The `gdf2zarrconverter` function converts spatial data from a GeoDataFrame into a Zarr store.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Data Cleaning: Ensures uniformity by cleaning various input data types.\n",
    "2. Spatial Extent Determination: Calculates the bounding box of the GeoDataFrame.\n",
    "3. Resolution and Dimensions Calculation: Determines raster resolution, width, and height based on spatial extent.\n",
    "4. Data Preparation: Separates columns into categorical and numerical, cleans missing data, and encodes categorical columns numerically.\n",
    "5. Rasterization: Converts categorical and numerical data into raster layers.\n",
    "6. Creating Xarray Dataset: Constructs a dataset to hold raster layers and category mappings, and sets latitude and longitude coordinates.\n",
    "7. Attributes and Metadata: Sets categorical encoding to each variable.\n",
    "8. Saving to Zarr: Saves the dataset to a Zarr store in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf2zarrconverter(file_path, native_var, title, layer, arco_asset_tmp_path, zipurl):\n",
    "\n",
    "    def cleaner(data):\n",
    "        if isinstance(data, str):\n",
    "            if data == '0' or data == ' ' or data == np.nan or data == 'nan' or data == \"\" or data == \" \":\n",
    "                data = 'None'\n",
    "        return data\n",
    "\n",
    "    def encode_categorical(data):\n",
    "        if isinstance(data[0], str):\n",
    "            data = pd.Series(data)\n",
    "            data = data.fillna('None')  # replace None values with 'None'\n",
    "            \n",
    "            data[data == ' '] = 'None'\n",
    "            data[data == '0'] = 'None'\n",
    "            data = data.values \n",
    "            unique_categories = np.unique(data)\n",
    "            category_mapping = {'None': 1}\n",
    "            counter = 2\n",
    "            for category in unique_categories:\n",
    "                if category != 'None':\n",
    "                    category_mapping[category] = counter\n",
    "                    counter += 1\n",
    "            encoded_data = np.array([category_mapping.get(item, np.nan) for item in data])\n",
    "        else:\n",
    "            encoded_data = data.astype(np.float32)\n",
    "            category_mapping = {}\n",
    "        return encoded_data, category_mapping\n",
    "\n",
    "    with fiona.open(file_path, 'r', layer=layer) as src:\n",
    "        crs = src.crs\n",
    "        total_bounds = src.bounds\n",
    "        lon_min, lat_min, lon_max, lat_max = total_bounds\n",
    "        resolution = 0.01\n",
    "        width = int(np.ceil((lon_max - lon_min) / resolution))\n",
    "        height = int(np.ceil((lat_max - lat_min) / resolution))\n",
    "        raster_transform = rasterio.transform.from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "        raster = np.zeros((height, width), dtype=np.float32)\n",
    "        data = []\n",
    "        geometries = []\n",
    "        with tqdm(total=len(src), desc=f\"Processing features of {layer} - {native_var}\") as pbar:\n",
    "            for feature in src:\n",
    "                value = cleaner(feature['properties'][native_var])\n",
    "                data.append(value)\n",
    "                geometries.append(feature['geometry'])\n",
    "                pbar.update()\n",
    "        data = np.array(data)\n",
    "        encoded_data, category_mapping = encode_categorical(data)\n",
    "        with tqdm(total=len(geometries), desc=\"Rasterizing\") as pbar:\n",
    "            rasterio.features.rasterize(\n",
    "                ((geom, value) for geom, value in zip(geometries, encoded_data)),\n",
    "                out=raster,\n",
    "                transform=raster_transform,\n",
    "                merge_alg=rasterio.enums.MergeAlg.replace,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            pbar.update()\n",
    "        \n",
    "        # make xarray dataset, arrange latitude from max to min since rasterio makes rasters from top left to bottom right\n",
    "        dataset = xr.Dataset(coords={'latitude':  np.round(np.linspace(lat_max, lat_min, height, dtype=float), decimals=4),\n",
    "                                    'longitude': np.round(np.linspace(lon_min, lon_max, width, dtype=float), decimals=4)})\n",
    "        dataset[native_var] = (['latitude', 'longitude'], raster)\n",
    "        dataset = dataset.sortby('latitude')\n",
    "\n",
    "        if category_mapping:\n",
    "            # save the mappig dictionary with the variable attributes\n",
    "            dataset[native_var].attrs['categorical_encoding']= category_mapping\n",
    "\n",
    "        dataset = attributes_update(dataset, title, resolution, zipurl)\n",
    "        zarr_var_path = f\"{arco_asset_tmp_path}/{title}_{native_var}.zarr\"\n",
    "        dataset.to_zarr(zarr_var_path, mode='w', consolidated=True)\n",
    "        return zarr_var_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5.  In this example we will use a multi-layer geodatabase featuring different layers of geological seabed substrate data taken from EMODnet Geology (https://emodnet.ec.europa.eu/geonetwork/srv/eng/catalog.search#/metadata/6eaf4c6bf28815e973b9c60aab5734e3ef9cd9c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMODnet_GEO_Seabed_Substrate_All_Res.zip: 777MB [01:12, 10.7MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import fiona\n",
    "import xarray as xr\n",
    "import dask\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Download the zip file\n",
    "zipurl = 'https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/emodnet_geology/seabed_substrate/multiscale_folk_5/EMODnet_GEO_Seabed_Substrate_All_Res.zip'\n",
    "geodatabase = 'EMODnet_Seabed_Substrate_1M.gdb'\n",
    "zip_file = os.path.basename(zipurl)\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=zip_file) as t:\n",
    "    urllib.request.urlretrieve(zipurl, filename=zip_file, reporthook=t.update_to)\n",
    "\n",
    "# Extract the geodatabase from the zip file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('extracted_files')\n",
    "\n",
    "for root, dirs, files in os.walk('extracted_files'):\n",
    "    for dir in dirs:\n",
    "        if dir.endswith('.gdb') and os.path.basename(dir) == geodatabase:\n",
    "            gdb_path = os.path.join(root, dir)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Geodatabase to zarr\n",
    "\n",
    " Geodatabases can often contain multiple layers, and often contain a number of columns(variables).\n",
    "\n",
    " We simplify the burden of conversion by converting layers and one variable from each layer into a zarr dataset. Then we combine them into a single zarr dataset using dask to rechunk the variables and ensure both compatibility of the zarr datasets, and lay the ground work for distributed computations if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Code: 100%|██████████| 33645/33645 [00:03<00:00, 10369.23it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<21:18:08,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Country: 100%|██████████| 33645/33645 [00:03<00:00, 10213.23it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:25:11,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Name: 100%|██████████| 33645/33645 [00:03<00:00, 9875.61it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:00:34,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Data_Holder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Data_Holder: 100%|██████████| 33645/33645 [00:03<00:00, 10695.55it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<21:04:40,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Contact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Contact: 100%|██████████| 33645/33645 [00:03<00:00, 10075.84it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<18:47:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Scale: 100%|██████████| 33645/33645 [00:03<00:00, 10276.32it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:52:19,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Original_Scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Original_Scale: 100%|██████████| 33645/33645 [00:03<00:00, 9207.93it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:01:22,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Original_Grain_Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Original_Grain_Size: 100%|██████████| 33645/33645 [00:03<00:00, 9822.53it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:02<21:18:47,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Mapping_Method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Mapping_Method: 100%|██████████| 33645/33645 [00:03<00:00, 10033.77it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:20:54,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - References\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - References: 100%|██████████| 33645/33645 [00:03<00:00, 9747.00it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:22:17,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Comments: 100%|██████████| 33645/33645 [00:03<00:00, 10026.67it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:50:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Reclassification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Reclassification: 100%|██████████| 33645/33645 [00:03<00:00, 9892.20it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:11:05,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Method: 100%|██████████| 33645/33645 [00:03<00:00, 9601.68it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:55:27,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Sample_number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Sample_number: 100%|██████████| 33645/33645 [00:03<00:00, 10484.89it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:23:39,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Original_substrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Original_substrate: 100%|██████████| 33645/33645 [00:03<00:00, 9323.22it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:35:02,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Relation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Relation: 100%|██████████| 33645/33645 [00:03<00:00, 9705.02it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:37:56,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_16cl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_16cl: 100%|██████████| 33645/33645 [00:03<00:00, 10520.65it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<22:02:23,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_16cl_txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_16cl_txt: 100%|██████████| 33645/33645 [00:03<00:00, 9133.11it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:20:25,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_7cl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_7cl: 100%|██████████| 33645/33645 [00:03<00:00, 8775.83it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:25:26,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_7cl_txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_7cl_txt: 100%|██████████| 33645/33645 [00:03<00:00, 10013.97it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:52:57,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_5cl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_5cl: 100%|██████████| 33645/33645 [00:03<00:00, 9619.27it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:11:51,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Folk_5cl_txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Folk_5cl_txt: 100%|██████████| 33645/33645 [00:03<00:00, 9145.59it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:01:40,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Surface_feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Surface_feature: 100%|██████████| 33645/33645 [00:03<00:00, 10299.56it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<21:31:13,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Surface_feature_Group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Surface_feature_Group: 100%|██████████| 33645/33645 [00:03<00:00, 9875.52it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:20:12,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Conf_RS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Conf_RS: 100%|██████████| 33645/33645 [00:03<00:00, 10057.75it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:04:07,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Conf_S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Conf_S: 100%|██████████| 33645/33645 [00:03<00:00, 9506.80it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<17:55:49,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Conf_D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Conf_D: 100%|██████████| 33645/33645 [00:03<00:00, 9134.10it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:30:04,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - Conf_TOT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - Conf_TOT: 100%|██████████| 33645/33645 [00:03<00:00, 10001.35it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:02<22:35:16,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - SHAPE_Length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - SHAPE_Length: 100%|██████████| 33645/33645 [00:03<00:00, 9043.96it/s] \n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:33:40,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Seabed_substrate_1M_Sep2023 - SHAPE_Area\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features of Seabed_substrate_1M_Sep2023 - SHAPE_Area: 100%|██████████| 33645/33645 [00:03<00:00, 10072.08it/s]\n",
      "Rasterizing:   0%|          | 1/33645 [00:01<18:20:39,  1.96s/it]\n",
      "/home/samwork/Documents/coding/vector_to_zarr/.venv/lib/python3.10/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'netcdf4' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 14GB\n",
      "Dimensions:                (latitude: 7395, longitude: 15675)\n",
      "Coordinates:\n",
      "  * latitude               (latitude) float64 59kB 7.903 7.913 ... 81.84 81.85\n",
      "  * longitude              (longitude) float64 125kB -88.74 -88.73 ... 68.0\n",
      "Data variables: (12/30)\n",
      "    Code                   (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Country                (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Name                   (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Data_Holder            (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Contact                (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Scale                  (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    ...                     ...\n",
      "    Conf_RS                (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Conf_S                 (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Conf_D                 (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    Conf_TOT               (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    SHAPE_Length           (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "    SHAPE_Area             (latitude, longitude) float32 464MB dask.array<chunksize=(3704, 7840), meta=np.ndarray>\n",
      "Attributes:\n",
      "    categorical_encoding:  {'Code': {'AZ': 2, 'BE-006': 3, 'BG-001': 4, 'CY-0...\n"
     ]
    }
   ],
   "source": [
    "temp_zarr_path = 'converted_zarr_files'\n",
    "\n",
    "os.makedirs(temp_zarr_path, exist_ok=True)\n",
    "title = os.path.splitext(os.path.basename(geodatabase))[0]\n",
    "\n",
    "# Get the layers from the geodatabase\n",
    "layers = fiona.listlayers(gdb_path)\n",
    "\n",
    "# Create an empty xarray dataset to hold the combined data\n",
    "combined_dataset = xr.Dataset()\n",
    "\n",
    "# Process each layer and each variable using gdf2zarr\n",
    "for layer in layers:\n",
    "    # Get the variables from the layer\n",
    "    variables = fiona.open(gdb_path, layer=layer).meta['schema']['properties'].keys()\n",
    "    \n",
    "    zarr_vars_paths = [] # replace with your column names\n",
    "    for variable in variables:\n",
    "        try:\n",
    "            print(f\"Processing {layer} - {variable}\")\n",
    "            zarr_var_path = gdf2zarrconverter(gdb_path, variable, title, layer, temp_zarr_path, zipurl)\n",
    "            zarr_vars_paths.append(zarr_var_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {layer} - {variable}: {e}\")\n",
    "            continue\n",
    "\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        for path in zarr_vars_paths:\n",
    "            try:\n",
    "                dataset = xr.open_dataset(path, chunks={})  # Use Dask to lazily load the dataset\n",
    "                dataset = dataset.chunk({'latitude': 'auto', 'longitude': 'auto'}) \n",
    "                combined_dataset = xr.merge([combined_dataset, dataset], compat='override', join='outer')\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to combine zarr dataset {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # add applicable categorical encodings\n",
    "    categorical_encodings_dict = {}\n",
    "    for var in combined_dataset.variables:\n",
    "        if 'categorical_encoding' in combined_dataset[var].attrs:\n",
    "            categorical_encodings_dict[var] = combined_dataset[var].attrs['categorical_encoding']\n",
    "\n",
    "    combined_dataset.attrs['categorical_encoding'] = categorical_encodings_dict\n",
    "\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        try:    \n",
    "            final_dataset = combined_dataset.chunk({'latitude': 'auto', 'longitude': 'auto'})  # for var in dataset.variables:\n",
    "            zarr_path = f\"{layer}.zarr\"\n",
    "            final_dataset.to_zarr(zarr_path, mode = 'w')\n",
    "            shutil.rmtree(temp_zarr_path)\n",
    "        except Exception as e:\n",
    "            print(f\"final zarr dataset did not save {layer}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Print the combined dataset\n",
    "print(combined_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edito-pipeline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
