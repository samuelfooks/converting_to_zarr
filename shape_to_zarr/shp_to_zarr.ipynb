{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rasterio.features\n",
    "import rasterio.transform\n",
    "import os\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import stat\n",
    "import os\n",
    "import sys\n",
    "import dask\n",
    "from datetime import datetime, date\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "from shapely.geometry import box, shape\n",
    "from tempfile import TemporaryDirectory\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attributes_update(dataset, title, resolution, zip_url):\n",
    "        latitudeattrs = {'_CoordinateAxisType': 'Lat', \n",
    "                            'axis': 'Y', \n",
    "                            'long_name': 'latitude', \n",
    "                            'max': dataset.latitude.values.max(), \n",
    "                            'min': dataset.latitude.values.min(), \n",
    "                            'standard_name': 'latitude', \n",
    "                            'step': (dataset.latitude.values.max() - dataset.latitude.values.min()) / dataset.latitude.values.shape[0], \n",
    "                            'units': 'degrees_north'\n",
    "            }\n",
    "        longitudeattrs = {'_CoordinateAxisType': 'Lon', \n",
    "                        'axis': 'X', \n",
    "                        'long_name': 'longitude',\n",
    "                        'max': dataset.longitude.values.max(),\n",
    "                        'min': dataset.longitude.values.min(),\n",
    "                        'standard_name': 'longitude', \n",
    "                        'step': (dataset.longitude.values.max() - dataset.longitude.values.min()) / dataset.longitude.values.shape[0], \n",
    "                        'units': 'degrees_east'\n",
    "        }\n",
    "        dataset.latitude.attrs.update(latitudeattrs)\n",
    "        dataset.longitude.attrs.update(longitudeattrs)\n",
    "\n",
    "        # Set the CRS as an attribute\n",
    "        dataset.attrs['proj:epsg'] = 4326\n",
    "        dataset.attrs['resolution'] = resolution\n",
    "        dataset.attrs.update({\n",
    "            'geospatial_lat_min': dataset['latitude'].min().item(),\n",
    "            'geospatial_lat_max': dataset['latitude'].max().item(),\n",
    "            'geospatial_lon_min': dataset['longitude'].min().item(),\n",
    "            'geospatial_lon_max': dataset['longitude'].max().item()\n",
    "        })\n",
    "        dataset.attrs['history'] = f'Converted on {date.today()}'\n",
    "        dataset.attrs['title'] = title\n",
    "      \n",
    "        dataset.attrs['Comment'] = f\"Downloaded from {zip_url} Converted from data product {title}.tif on {datetime.today()}\"\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download zipfile containing Shapefile for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(zip_url, target_shapefile):\n",
    "    zip_file_path = 'zipfiles/temp.zip'\n",
    "    # Download the zip file\n",
    "    response = requests.get(zip_url)\n",
    "    with open(zip_file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Extract the contents of the zip file\n",
    "    with ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall('zipfiles')\n",
    "    # Find the .shp file\n",
    "    shp_file_path = None\n",
    "    for file in os.listdir('zipfiles'):\n",
    "        if file.endswith(\".shp\") and os.path.basename(file) == target_shapefile:\n",
    "            shp_file_path = os.path.join('zipfiles', file)\n",
    "            break\n",
    "    return shp_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `gdf2zarrconverter`\n",
    "\n",
    "The function first defines two helper functions, `cleaner` and `encode_categorical`. `cleaner` is used to clean the data by replacing certain values with 'None'. `encode_categorical` is used to encode categorical data into numerical values.\n",
    "\n",
    "The function then opens the file at `file_path` and reads its contents. It calculates the bounds and resolution of the data, and creates an empty raster of the appropriate size.\n",
    "\n",
    "Next, the function iterates over the features in the source data. For each feature, it cleans the data and appends it to a list. It also appends the feature's geometry to a separate list.\n",
    "\n",
    "The data is then encoded using the `encode_categorical` function. The geometries and encoded data are used to rasterize the data.\n",
    "\n",
    "An xarray dataset is created from the raster, and the latitude is sorted. If there is a category mapping, it is saved as an attribute of the dataset.\n",
    "\n",
    "Finally, the dataset is updated with the `attributes_update` function, saved as a Zarr array at `zarr_var_path`, and the path is returned.\n",
    "\n",
    "```python\n",
    "def gdf2zarrconverter(file_path, native_var, title, layer, arco_asset_tmp_path, zipurl):\n",
    "    ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf2zarrconverter(shp_file_path, variable, resolution, arco_asset_tmp_path, zip_url):\n",
    "    def cleaner(data):\n",
    "        if isinstance(data, str):\n",
    "            if data == '0' or data =='' or data == np.nan or data == 'nan' or data == \"\" or data == \" \":\n",
    "                data = 'None'\n",
    "        return data\n",
    "\n",
    "    def encode_categorical(data):\n",
    "        if isinstance(data[0], str):\n",
    "            data[data ==''] = 'None'\n",
    "            data[data == '0'] = 'None'\n",
    "            unique_categories = np.unique(data)\n",
    "            category_mapping = {'None': 1}\n",
    "            counter = 2\n",
    "            for category in unique_categories:\n",
    "                if category!= 'None':\n",
    "                    category_mapping[category] = counter\n",
    "                    counter += 1\n",
    "            encoded_data = np.array([category_mapping.get(item, np.nan) for item in data])\n",
    "        else:\n",
    "            encoded_data = data.astype(np.float32)\n",
    "            category_mapping = {}\n",
    "        return encoded_data, category_mapping\n",
    "\n",
    "    resolution = float(resolution)\n",
    "    title = os.path.splitext(os.path.basename(shp_file_path))[0]\n",
    "    with fiona.open(shp_file_path, 'r') as src:\n",
    "        crs = src.crs\n",
    "        total_bounds = src.bounds\n",
    "        lon_min, lat_min, lon_max, lat_max = total_bounds\n",
    "        width = int(np.ceil((lon_max - lon_min) / resolution))\n",
    "        height = int(np.ceil((lat_max - lat_min) / resolution))\n",
    "        raster_transform = rasterio.transform.from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "        raster = np.zeros((height, width), dtype=np.float32)\n",
    "        data = []\n",
    "        geometries = []\n",
    "        with tqdm(total=len(src), desc=f\"Processing features of {variable}\") as pbar:\n",
    "            for feature in src:\n",
    "                value = cleaner(feature['properties'][variable])\n",
    "                data.append(value)\n",
    "                geometries.append(feature['geometry'])\n",
    "                pbar.update()\n",
    "        data = np.array(data)\n",
    "        encoded_data, category_mapping = encode_categorical(data)\n",
    "\n",
    "    # Create a grid for each GDF\n",
    "    grid = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    # Iterate over each polygon and overlay it on the grid\n",
    "    with tqdm(total=len(geometries), desc=f\"Rasterizing geometries\") as pbar:\n",
    "        for geom, value in zip(geometries, encoded_data):\n",
    "            geom = shape(geom)\n",
    "            # Rasterize the polygon on the grid\n",
    "            rasterio.features.rasterize(\n",
    "                [(geom, value)],\n",
    "                out=grid,\n",
    "                transform=raster_transform,\n",
    "                merge_alg=rasterio.enums.MergeAlg.replace,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            pbar.update()\n",
    "    chunk_size = 200\n",
    "    # Create an xarray dataset from the grid\n",
    "    dataset = xr.Dataset(coords={'latitude':  np.round(np.linspace(lat_max, lat_min, height, dtype=float), decimals=4),\n",
    "                                 'longitude': np.round(np.linspace(lon_min, lon_max, width, dtype=float), decimals=4)})\n",
    "    dataset[variable] = (['latitude', 'longitude'], grid)\n",
    "    \n",
    "    # first chunk for the heavy sorting operation\n",
    "    dataset = dataset.chunk({'latitude': chunk_size, 'longitude': chunk_size})\n",
    "    dataset = dataset.sortby('latitude')\n",
    "\n",
    "    # rechunk to corrects even sorted latitude chunks\n",
    "    dataset = dataset.chunk({'latitude': chunk_size, 'longitude': chunk_size})\n",
    "    \n",
    "    if category_mapping:\n",
    "        # save the mappig dictionary with the variable attributes\n",
    "        dataset[variable].attrs['categorical_encoding']= category_mapping\n",
    "\n",
    "    dataset = attributes_update(dataset, title, resolution, zip_url)\n",
    "\n",
    "    zarr_var_path = f\"{arco_asset_tmp_path}/{title}_{variable}.zarr\"\n",
    "    dataset.to_zarr(zarr_var_path, mode='w', consolidated=True)\n",
    "    return zarr_var_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Zarr\n",
    "\n",
    "Convert each variable of the Shapefile into Zarr format, using the gdf2zarrconverter.  This is to minimize memory consumption during the conversion process.  Then each dataset is rechunked and combined into a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zipdir = 'zipfiles'\n",
    "os.makedirs(zipdir, exist_ok=True)\n",
    "zip_url = \"https://s3.waw3-1.cloudferro.com/emodnet/emodnet_native/archive/human_activities_windfarms/EMODnet_HA_Energy_WindFarms_polygons_20231124.zip\"\n",
    "shapefile = 'EMODnet_HA_Energy_WindFarms_pg_20231124.shp'\n",
    "arco_asset_temp_dir = 'data'\n",
    "resolution = \"0.01\" \n",
    "\n",
    "# Download and extract the zip file, then get the path to the .shp file\n",
    "shp_file_path = download_and_extract_zip(zip_url)\n",
    "\n",
    "print(shp_file_path)\n",
    "permissions = stat.filemode(os.stat(shp_file_path).st_mode)\n",
    "print(\"File Permissions:\", permissions)\n",
    "# Convert the .shp file to zarr using gdf2zarrconverter function\n",
    "if shp_file_path:\n",
    "    \n",
    "    title = os.path.splitext(os.path.basename(shp_file_path))[0]\n",
    "    combined_dataset = xr.Dataset()\n",
    "    \n",
    "    variables = fiona.open(shp_file_path).meta['schema']['properties'].keys()\n",
    "\n",
    "    zarr_vars_paths = []\n",
    "    # Process each variable in the .shp file or choose your own\n",
    "    for variable in variables:\n",
    "        try:\n",
    "            print(f\"Processing {variable}\")\n",
    "            zarr_var_path = gdf2zarrconverter(shp_file_path, variable, resolution, arco_asset_temp_dir, zip_url )\n",
    "            zarr_vars_paths.append(zarr_var_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {variable}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # join all the zarr datasets into a single dataset\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        for path in zarr_vars_paths:\n",
    "            try:\n",
    "                dataset = xr.open_dataset(path, chunks={})  # Use Dask to lazily load the dataset\n",
    "                dataset = dataset.chunk({'latitude': 'auto', 'longitude': 'auto'}) \n",
    "                combined_dataset = xr.merge([combined_dataset, dataset], compat='override', join='outer')\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to combine zarr dataset {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # add applicable categorical encodings\n",
    "    categorical_encodings_dict = {}\n",
    "    for var in combined_dataset.variables:\n",
    "        if 'categorical_encoding' in combined_dataset[var].attrs:\n",
    "            categorical_encodings_dict[var] = combined_dataset[var].attrs['categorical_encoding']\n",
    "\n",
    "    combined_dataset.attrs['categorical_encoding'] = categorical_encodings_dict\n",
    "\n",
    "    # rechunk and save the final dataset\n",
    "    with dask.config.set(scheduler='single-threaded'):\n",
    "        try:    \n",
    "            final_dataset = combined_dataset.chunk({'latitude': 'auto', 'longitude': 'auto'})  # for var in dataset.variables:\n",
    "            combined_zarr = f\"{title}_res{resolution}.zarr\"\n",
    "            final_dataset.to_zarr(f\"{arco_asset_temp_dir}/{combined_zarr}\", mode = 'w')\n",
    "\n",
    "            # Cleanup: delete all zarr files except the final one\n",
    "            for file in os.listdir(arco_asset_temp_dir):\n",
    "                if file.endswith(\".zarr\") and file != combined_zarr:\n",
    "                    shutil.rmtree(os.path.join(arco_asset_temp_dir, file))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"final zarr dataset did not save {title}: {e}\")\n",
    "\n",
    "# Print the combined dataset\n",
    "print(combined_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
